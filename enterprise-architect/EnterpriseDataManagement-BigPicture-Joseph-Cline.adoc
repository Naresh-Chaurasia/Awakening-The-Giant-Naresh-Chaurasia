= EDM
:toc: left
:toclevels: 5
:sectnums:
:sectnumlevels: 5

NOTE: NOTE

TIP: TIP

IMPORTANT: IMPORTANT

CAUTION: CAUTION

WARNING: WARNING

[cols="1,3"]
|===
| Name | Description

| Asciidoctor
| Awesome way to write documentation

|===

== Big Picture: Enterprise Data Management by Joseph Cline

*Maximizing Value*

* *Fundamentals*: Learn the concepts
* *Information*: Do POC from the course
* *Skills*: Use / Implement it in Project
* *Innovation / Expert*: Value for next 2 Years


*Objective*

* History:
* Agenda-While doing this course what is the Agenda:
** 18.May.2022
*** 1st Iteration. Make important information as *bold* and #yellow highlight#
*** Focus on KeyBank Interview
*** After completing the course, work with Jonathan Levi

* Exam Notes
* *Why OneNote Notes*:
* *What is Pending*:

=== Course Overview

*Course Overview* - Ok. Don't waste your time. Please.

Hi everyone. My name is Joe Cline, and welcome to my course, Big Picture: Enterprise Data Management. Have you ever the phrase knowledge is power? Well you can't have knowledge without information nor get information without data. Over the last decade, technology has given us the ability to generate, obtain, and learn from data in more ways than ever before. So I ask you this. Are you ready to take a high-level survey of this exciting, but often misunderstood field? That's great because some of the major topics we will cover in this course will include the various disciplines within enterprise data management, the roles, responsibilities, methodologies, and technologies for each of these disciplines, and talk about some of the hot technologies and topics, such as master data management, big data, and blockchain in the enterprise. This is an introductory course, so no experience is necessary. But after completing this course, you should feel comfortable diving deeper into the pool of enterprise data management with courses on Enterprise Data Governance, Enterprise Data Architecture and Master Data Management, Data Modeling, and others. I hope you'll join me on this journey to learn about managing data with the Big Picture: Enterprise Data Management course only at Pluralsight.

=== Course Introduction

*Introduction* - Ok. Don't waste your time. Please.

Hello and welcome to my course, Big Picture: Enterprise Data management. This is an introductory course to managing data on an enterprise scale. Since this is an intro course, no experience is assumed. My name is Joe Cline, and so you'll have some confidence in the quality of information you'll get from this course, check out my LinkedIn page. Or better yet, send me an invite to connect. Any time you have a question about the course or data management in general, shoot me a tweet @mrjoedata. You might be thinking, hey Joe so what am I going to get out of this course? Well like I said, this is an *introduction course to data management on an enterprise scale*. What do I mean by enterprise? Well, enterprise is the term often used to refer to the entirety of an organization or the organization's technical infrastructure. And because this is an introduction course, we'll be looking at the data management aspect of the enterprise from a 50, 000 foot view. Because each organization is unique, I can't give you one solution to implement in any one of the areas we'll learn about, but this course is the beginning to understanding the roles, disciplines, methodologies, and technologies common in a data-driven organization. Let's go onto the next video where I'll introduce you to the concept of the data lifecycle.

*The Data Lifecycle* - #Pending#

> Lot of overwhelming information to consume.

Now in reality, the life of data isn't always linear, and what I mean is we don't always create data, do something with it, and then just purge it. Often we transform data. We give it new meaning. And so in those cases, data is cyclical. Here we have the DAMA Data Lifecycle Wheel. DAMA is an international organization solely dedicated to data management and all it entails. In fact, they publish a book called DMBOK, which stands for Data Management Body of Knowledge. This image here is taken from the DMBOK. DAMA also produces a data management dictionary. So do yourself a favor and go check them out. Data has a lifecycle, and according to DAMA it can be pretty complex. But notice one thing. Even with all the parts broken into their own little sections, one thing that holds them all together is data governance. It's difficult to describe how things work when they move in a circular way. It's one of those which-comes-first scenarios because data comes from all around us. Some is legacy, some we create, and some we extract. But I want to keep this easy to understand, so I'll do my best to describe it as if it were a linear process. There are a lot of sections in DAMA's wheel, and some of these might be a little too specific for this introductory course. So I decided to group these sections together into three areas. Preparing a Data Strategy will cover governance, architecture, and modeling. In Managing and Working with Data, we'll talk about engineering and administration. And finally in Getting Value from Data, we'll get into business intelligence, analytics, and predictive modeling. In the next module, we'll start with developing a data strategy, which will include implementing data governance, defining master data management, enterprise data architecture, and how we can capture it all in our enterprise data model. We'll also look at roles associated with the disciplines in this area, the CDO, or chief data officer, data governance directors, data stewards, or subject matter experts, enterprise data, or sometimes called information architects, and data modelers. In another module, we'll get into working with data in various ways, to organize, persist, secure, profile, transform, and analyze it. We'll also talk about the relational database management system and the structured query language, or SQL. After that, we'll get into discussing the roles for this area as in database administrators, or DBAs, data developers, data integration developers or sometimes called ETL developers. Then we'll touch on the importance of data profiling when it comes to data quality. Then finally, we'll get into business intelligence, analytics, and predictive modeling to gain insight into your company's data and attempt to predict outcomes of decisions based on existing data patterns. The roles in this area are data engineers, or sometimes big data engineers, business intelligence developers, data, statistical data, and predictive modeling analyst, and ending with data visualization developers. By the end of this course, you will have a solid understanding of enterprise data management including the enterprise data architecture and its many components like master data management. We'll even touch a little on blockchain technology and how that might fit in. You'll also be able to describe job titles and their responsibilities within the larger data architecture, and hopefully you'll already be contemplating a data strategy of your own. I'll see you in the next module.

Preparing an Enterprise Data Strategy
Module Introduction
Hi, and welcome back to Big Picture: Enterprise Data Management. I'm Joe Cline. In the course introduction, we briefly talked about the DAMA wheel and the steps of the data lifecycle. We also talked about how the data lifecycle is cyclical and can be a little complicated for an introductory course, so I broke the steps out into three different parts. In this module, we'll discuss the first part: preparing an enterprise data strategy, focusing on the disciplines around governance, architecture, and modeling. Let's get started.

Relationship with Data Over Time
When we talk of preparing an enterprise data strategy, we're actually talking about building a data-oriented culture. Before we get into it and to really appreciate the value of data, we should take a look at the relationship organizations have had with it as technology has developed over the years. In the 1960s, networking between two computers was invented by DARPA. A few companies, like Bell Labs and IBM, eventually developed their own networks. If you think of data as the blood force of a company, network are the arteries that the data runs through because without the ability to share and comingle data across systems, it would be difficult to know its potential value. In 1970, E.F. Codd invented the relational data model at IBM, but most organizations were already heavily invested in mainframes, also developed by IBM, and so stuck with their reels of magnetic data tape instead of moving on to a database. At the end of the 70s and as we move into the 80s, we saw the advent of the personal computer. Apple had created its first computer, and IBM soon followed with their PC. Companies began to see the value in empowering their employees with desktop computers. As desktop computers increased with popularity with businesses, new computer manufacturers began to pop up creating the IBM or pc clone. Server client network topology was developed, and suddenly people could send messages to each other and access files on each other's computers, as well as fileservers right from their own desk. These new servers had the UNIX operating system invented by AT and T in the 60s. IBM also developed servers as an alternative to expensive mainframes. The servers were still big by today's standards, but weren't as expensive and had much more computing capability. Some companies continued with the mainframe to store company data and ran a server/client network for office automation and departmental data. This was the beginning of what some called Excel Hell because data was kept in spreadsheets, and copies of it were saved all over the network. People began working without data versions because there wasn't any type of version control or central database for them to use instead. Relational database systems hit the market in the 70s, but really started to take off in the 80s. The most popular database systems were from Oracle and IBM who developed them to run on the huge sun servers, while Xerox helped companies be more efficient with copiers and other office machines. By the late 80s, we weren't just automating office systems, we were developing software to help run the business. For example, travel agents could view inventory and make reservations for air travel and hotel stays by connecting to specialized private networks over a modem. Hardware became cheaper and smaller, and companies began figuring out ways to automate their own business processes. As we moved into the 90s, Microsoft introduced a GUI version of their own DOS operating system called Windows. While Apple had released the Macintosh in 1984 with a GUI operating system, their systems were more expensive than the PC clones and marketed for artistic work like desktop publishing and computer graphics. The interface was more intuitive, and it made it easier for more and more people to learn how to use computer software. As the clone wars continued, that is the computer manufacturers I mentioned earlier, started putting the Windows operating system on all their computers, which pushed the price way down as competition grew, making Windows the de facto corporate standard. By the late 90s, it was all about the World Wide Web and the internet. Every company spent time and money to figure out how they could benefit from being online. This is where data started to take a backseat to using technology for productivity, office automation, and having that virtual presence. However, companies still generated a lot of data and needed to save it somewhere. Data warehouses began showing up at more and more companies, but data was still mostly used for basic reporting, being viewed as a byproduct of software, and databases were simply a place that developers could store their data for later use. Once the 2000s rolled in and companies got a better handle on their internet presence, new buzz phrases, business intelligence, golden record, single version of the truth, were being tatted as a the silver bullet to all data problems. Data warehousing started to become important. And how that data was organized did too. The cost of disk or storage continued to drop. Data warehouses were being modeled by data warehouse architects, and BI developers helped business analysts to slice and drill down with the implementation of an OLAP cube. At this point, database servers could run on commodity hardware, which was also cheap, but didn't have a way to scale efficiently. DBAs tried database farms and peer-to-peer replication, but even as they got the servers to work with each other, the network became the bottleneck. Around 2004, the term big data rose in popularity and was at the beginning of its hype cycle. Companies started to worry their database servers couldn't handle it all, but they couldn't scale their database commodity servers either and worried they were going to have to go back to the big UNIX servers, so they started looking into big data solutions. Hadoop was created to address that problem with HDFS, the Hadoop distributed filesystem and the map reduced algorithm. Finally, data was important again. Now it was a company asset. And what do companies do with assets? They put controls around them. In this case, those controls were data governance. Now data governance has been around in some form or another since the 1980s, but most companies have not learned how to do it well until the 2000s. In the next video, I'll get into what data governance is and how it fits into enterprise data management, as well as the roles of this important discipline. I'll also suggest how you might want to implement a data governance program in your organization.

Discovering Data Governance Part 1
Now data governance is different than the other disciplines I discuss in this course. It's different because it's not a technical or a design skill so much as it is a set of practices, and it's never really finished because even after the project is declared done done, sorry, scrum joke, governance policies persist. We'll discuss data governance as a top-down center of excellence because that has so far been a successful model for many companies. Now you can try a bottom-up methodology, but honestly I haven't heard of one getting much traction. So governance primary function is to develop policy and standards. As I've said before, it not really a step in the lifecycle like data architecture or data administration because it has to be woven throughout. Decisions are made by a committee and never in a vacuum. It's like the HR Department for data. HR is also not like other departments in the company. I mean they exist to make sure everyone follows the rules that keep employees being the most productive they can be and not get the company sued. So now that you know that it's a different kind of thing, let me tell you how it's a different kind of thing. So here's some examples of what a data governance program would be responsible for: first, establishing and maintaining a corporate glossary, then, establishing and maintaining an enterprise data dictionary, naming conventions for the names of databases, tables, and column names, etc., information security, change management, and facilitate at least the initial interaction between data stewards and data modelers, and in some cases you might have to do a little data project management. So let's go through each one of these points one by one. A corporate glossary is a document defining the already agreed-upon terminology used in the course of regular business. The terminology defined in the glossary are words regularly used by the industry to describe things like entities, transactions, or metrics. And we do this because we want everyone in our organization to speak the same language. Yes, I know, I know, you're thinking Joe, isn't that just called a data dictionary? Well, no. The difference is a corporate glossary exists for the corporation, where the data dictionary is tied to the enterprise data model. Speaking of enterprise data dictionary, well it's the mother of all data dictionaries. It is the superset of all defined data in the enterprise. Unlike the glossary that only has the business term and its definition, the enterprise data dictionary describes the metadata of those terms in the glossary as they are documented in the conceptual and logical enterprise data model. An enterprise data dictionary will tell you the name and description just like the corporate glossary, but here's where it gets a step further. It'll also tell you the data domain or, in the physical terms, data type of that particular attribute. There will also be a description of what kind of data, maybe even an example of the type of data, that would probably be in that field. And there might even be a notes field, and this would be a good place to tag that attribute with the name of the compliance law that it's subjected to. Naming conventions define the rules around naming data objects and their attributes. For example, say you want to name a database or an entity or its physical counterpart, a table, or the table's columns, how do you want those words spelled out? Or do you want them abbreviated? If abbreviations are allowed, should they be standardized? And here's a hint, yes. In fact, you will be documenting these naming conventions in your enterprise data model. If the name of your data object needs more than one word, how do you separate the words knowing spaces aren't a good idea, at least not in the physical model. What's the difference between naming the same object in the logical model and the physical model? Do you separate the words with underscores, or do you use Camel Case? Are your object names capitalized? Is each word capitalized or only the first, that is Pascal Case or upper Camel Case. What about the names for the attributes of the object? These are the types of things that you'll decide on in your data governance program. Now that we have some of that housecleaning out of the way, let's talk security. Now when I say information security, I don't mean that your data governance people should be hackers or pen testers. You should already have a separate department for cybersecurity. What I'm talking about is identifying data or its attributes for auditing of federal or industry compliance laws, such as PII, Sarbanes-Oxley, and HIPAA to name a few. That way it's easy to search for when an audit comes around, and it always comes around. The data governance practice should also be in close and regular communication with your InfoSec or CyberSec department in your organization. In fact, you should bring someone from that department in as a subject matter expert, but I'll get into what a subject matter expert is when I get into the roles of the data governance program. Now change management. If you're not familiar with that, that means that no data, no data schema, no database server, no networking, no storage, no cables, nothing gets updated, deleted, or otherwise changed without the change being documented and approval given. Now this can be managed by a separate change management panel within your data governance program, or it can be a part of an existing IT governance council. Sometimes the governance program facilities the initial meeting between the business requester and the data modeler and developer. They can't even act as a stakeholder or a project manager for some of these data development projects. Okay, we now know what a data governance program does, but who does it?

Discovering Data Governance Part 2
First, there should be an executive in charge with enough influence that everyone involved takes it seriously. Sometimes that role is called a CDO, or chief data officer. But if you don't have one, get someone at the VP level to fill that role. Then you'll need someone to act as the program director and preferably with a team of data governance coordinators because there should be a lot of work. Other roles may actually be a position in the company or if not probably part of an existing job description. Data stewards are the stakeholders, or owners, of the data in question. A data steward should be a person on the business side who owns a system and is very familiar with its data. Nothing should change in their subject domain without their approval. Data stewards should, in my opinion, always be from the business and not IT. Next we have the SME, S-M-E, or subject matter expert, Like someone from the cybersecurity department I was talking about earlier whose an expert in security and can answer questions around that. A database administrator can be a SME or the head of global sales can be a SME. A data steward can also be a SME. So a SME is basically anyone who knows a lot about something and can explain that something in detail. It doesn't matter what the SME's title is or where the SME works as long as they can be a consistent resource for the governance council. The enterprise information architect doesn't usually report to the data governance program, at least not directly, but an enterprise data modeler might. Okay as promised, here is, in a nutshell, a list of things you can do to create a data governance center of excellence style program in your organization. The first step, and this is crucial. That's why I'm saying it again here. Get executive sponsorship. Now maybe this is you. Or if your organization has a CDO or a VP of data, get them to sponsor it. Of course, these are busy people, so you'll need to convince them to take on the role by volunteering to take on all the necessary footwork. Then they can have as much involvement as they want. But you need their clout and their budget. Then you'll need to obtain and learn to use modeling software. Now these can be expensive, which is another reason having an executive sponsor your program is important. Here are two modeling suites that I've used and are some of the most popular out there, ER Studio and ERWin. Either one will work. It just depends on what bells and whistles you want. Now you might be able to get away with something inexpensive or even free if you go the open source route. A couple of options there are ER One and Open ModelSphere. Just make sure they support the databases you use. Next you'll want to identify your data stewards and SMEs, get commitments from them, and help them feel honored to be asked to participate. It could provide them some professional visibility, so use that as a selling point. Remember how I said data governance is not done in a vacuum? Well, now that you have a team, that is a CDO, a governance director, and their team, data stewards and SMEs, you'll want to establish a data governance council and hold regular meetings. Some of the things the council will need to vote on are some of the stuff that we've already covered, like policies and agreeing on terms in your glossaries and data dictionaries and agreeing on naming conventions and reviewing and approving changes to anything that involves with the data tier in your enterprise. Also the data governance council might be the governing council of the change management panel. Next, you'll want to get involved with the development project before and while development is happening, Insert yourself or a data modeler into your development's teams morning standups to make sure whatever decision is being made about the data tier are adhering to data governance policy. Remember, stay firm, be tenacious, and don't give into whining. Once you give in, the integrity of your program begins to crumble away. Now I don't mean be the data police. Remember the HR comparison? Be like that. Be like how an HR person would approach a situation. In other words, be tactful and courteous, but don't give in. Remember this phrase because it'll help you out. Any changes to systems or policy must go through the data governance council. Now you might have a change request form or something like that that the person who wants to make the change can fill out and submit to the panel maybe a couple days before you actually meet, so everyone on the panel has time to review it before the next meeting. And ask the requestor to be there at that meeting, so they can answer any questions like, is there risk to other systems, and if there will be downtime, how will they communicate that to the end-users? You get the picture. Now you might be saying to yourself, Joe, hold on, wait. We just don't have the people to do all this. Okay, that just means that everyone will have to take on multiple roles. One thing you can do is to get the application developers or DBAs to be data modelers, and you can do that by holding lunch and learn sessions and teaching them about the importance of modeling, how to use a modeling tool, and how and when to normalize a table, etc., etc. If you are worried no one will show up to your session, make it a pizza party or cater in. Everyone loves free food. Just don't forget about the vegans and vegetarians. Okay, now that we have an understanding of what data governance is, let's talk enterprise data architecture.

Discovering Enterprise Data Architecture Part 1
Sometimes it's called data, sometimes information. You see them used interchangeably all the time, but are they really the same thing? Well, here's how I think of it. Data is a measurement or an attribute when put into context becomes information. And once that information is consumed, it becomes knowledge. And with experience, knowledge becomes wisdom. Let me confuse you with how enterprise architects use it. Now to talk about enterprise data architecture, we have to talk about enterprise architecture as a whole. One of the most popular EA frameworks is the open group's architecture framework, otherwise known as TOGAF. The open groups TOGAF framework uses the term information to describe one of its four pillars of the enterprise architecture. That's correct. It's the one that deals with data along with the business architecture, application architecture, and technology architecture. Boy, say that three times fast. The business architect has this model. And that has all the requirements in it. They pass that on to the information architect who will try to fit that into the larger enterprise data architecture and create a conceptual data model that satisfies the data tier aspect of the requirements outlined in the business model. This is then turned over to the application architect who will design a solution to fit it in with the greater software architecture, like service oriented architecture, and create a model with it. Then comes the technology architect who will look at what's been prosed, make determinations on where that solution fits into the enterprise infrastructure architecture, create some kind of network topology model, and then works with the network OS database administrators to implement. Obviously, you don't need to concern yourself with the other pillars in the framework because we're only going to focus on the information domain. And in my opinion, it's the best one. This will be a good time to introduce blockchain technology. I know what you're thinking? Hey Joe, isn't this just another name for bitcoin? Why in the world would you want to talk about bitcoin in an enterprise data management course? Well I'm not talking about bitcoin or cryptocurrency specifically, but the technology that the cryptocurrency runs on, which is called blockchain. Blockchain is a type of distributed ledger, which is essentially a decentralize and distributed database. And when I say distributed, I mean it's a peer-to-peer network where every peer is called a node, and each node has a copy of the blockchain, so they have the whole blockchain. It works something like this. Now keep in mind, this is a very generalized explanation because there are different types of distributed ledgers out there. The blockchain is all about removing a trust broker or the middle person from a transaction. A trust broker could be a bank or the Amazon Marketplace. Now the transaction can be just about anything. It could be your personal identification that you share with a business or use to fill out a job application or a loan. It can be a vote for an election or a smart contract of some kind. Let's look at how that might work. First, you have to have an agreement to do something. It could be a small as sending a bitcoin to a friend or as complex as a contract between a vendor and a client. The transaction is inputted into the DAP on the device where it's encrypted. That encrypted value called a block, is broadcast to the blockchain network. In order for this to work, a node has to pick up the transaction and run some algorithm against it to make sure that it's a valid transaction and not a fraudulent hack. This is known as coin or token mining. That is, you may the node operator cryptocurrency to validate your block. The bigger the block, the more validations, the more it'll cost in cryptocurrency. Once validated, the block is broadcast again to the blockchain. Now all distributed ledgers use what's called a consensus algorithm. A consensus algorithm is used so that enough nodes on the network can agree that a transaction is valid before it gets added to the blockchain. The algorithm is different depending on what type of distributed ledger you're working with. So now you have one node that validated your transaction, it gets broadcast back out to the blockchain network, and other nodes will pick it up. And those who do, you'll pay them some cryptocoin. So several nodes will have to validate your transaction. I think usually it's three is the minimum. Now depending on the size, it may only take that minimum of 3 nodes, or it may take 20 nodes, or it may take 100 nodes to validate it before it is added to the chain. This can take several minutes or several days, again depending on the size of the transaction and the blockchain's network latency. When added to the chain, the transaction is documented in the ledger, and confirmation of the transaction is sent to the dap on your device. Again, this is a very high-level generalization. But why would a company have a blockchain in its enterprise data architecture? Well, more and more companies are seeing a benefit to using a blockchain for certain types of business processes where they usually contract a third party to manage for them, like customer relationship management or a loyalty points program. In fact, IBM and Microsoft have already developed blockchain solutions for business. The key thing to remember is you don't want to store actual data on the chain, only a record of the transaction, maybe included with a pointer to another database if necessary. Remember, the size of the transaction determines how much cryptocurrency you're going to pay to get it validated and how much time it's going to get validated before it's written to the blockchain. The transaction record could also be a type of metadata, which is only one type of data companies work with. Let's take a look at all of them.

Discovering Enterprise Data Architecture Part 2
Now some of the things that a data architect will do is to identify master data, transactional data, lookup data, and metadata and arrange it appropriately as a part of the enterprise data architecture for their organization. That arrangement can manifest physically or logically. When talking about master data, I'm referring to data that describes an entity, that is a person, place, or thing. A customer is a person and probably the most referenced example of master data management. Think customer relationship management systems or CRMs as an implementation of customer MDM. Now a place could be a hotel or a store location or cities. And a thing might be date that describes something like sensors for the internet of things. Generally, the attributes of things described as master data don't change or don't change often. Take, for example, a person's eye color or a building street address or a sensor's serial number. That's master data. A business process like a purchase in a store or a contract is what happens between two master data entities, like when a customer makes a purchase from a business or a guest checks into a hotel. The data describing that purchase is called transactional data. Now lookup data is just as it sounds. A lot of times, data in our tables is abstracted into codes or IDs that we have some kind of foreign key relationship to, and sometimes we need to look up the description of those codes or IDs so we know what they mean. Incidentally, the more normalized your data model, the more lookup entities you'll likely see. And then we have metadata, which is data about the other three types of data. For example, a data object, say like a table, has a description. We want to know what that table is about, so we read a description about it. That description is metadata for that table. And that table has attributes, and we want to know the data types for a certain attribute. Say we have a person table or a person entity, and we want to know the data type for the attribute called eye color. Well we see that eye color is a string, and the attribute for age would be a tiny integer. Now we know what types of data we have to work with and how we might want to design the data tier of our enterprise. Now let's get onto the enterprise data model.

Discovering the Enterprise Data Model
So why model data? Well we model to organize and define our data. Take a business and customer, for example. We want to be able to describe them both and not only them, but the interaction carried out between the two. In our model, we would describe our customer, business, and the interaction between them as entities. Now you can think of an entity like a noun in a sentence. It's a person, place, or thing. But what if a business has more than one type of customer? Let's use the hotel analogy for this. In the hotel industry, the big chain hotels don't own all the hotels with their brand. They create a franchise, so the individual hotel owners can operate like they're a big hotel corporation. The hotel company is selling its services to the hotel owner who will call a franchisee. That makes the franchisee a customer of the hotel company, but the people who stay at the hotel are also customers of the hotel company because they're buying the brand, and that's why they chose to stay at that particular hotel. So who are we talking about? It's important to define what a customer is so that there is no confusion about what you're trying to do. Fortunately, we did that in our data governance program, so we're covered. In our example, we'll call our customer a guest and our business a hotel, and the interaction made between them is called a transaction. So what type of data are we working with? Well, our guest and our hotel entities are master data, while our transaction entity is, that's right, transactional data. Now we need to be careful not to define a transaction too narrowly because a transaction is just a business process, and there are a lot of business processes that occur during the course of a guest's stay at a hotel. Before the guest even gets there, they go online and shop for a hotel. When they find one that meets their needs, they make a reservation. When they arrive at the hotel, they check in. Maybe the guest raids the minibar and racks up a bill, and in the morning they pay the bill and check out. These examples are all typical business processes for a guest stay at a hotel, and we want to capture them all in our model. So what does the data modeler do? Well, the data modeler works with the information architect and the business architect to further define their model. They start out with a conceptual model that doesn't usually get into attributes and types, but has a general description of the data requirement. So the data modeler takes that conceptual model and the results is the logical data model. Take a look at this ER diagram. ER stands for entity relationship, and ER diagram is a visual representation of your logical data model. The logical model is more detailed than the conceptual model because it has the entity's metadata. Remember what metadata is? Metadata describes other data, like the attributes and data domains, that is data types for an entity with a description, notes, default values, and more. This is also the place where you might tag whether that piece of data is subjected to compliance rules, like PII or Sarbanes-Oxley. It's in the logical model where we apply our normalization techniques. Once the logical model is approved by the business, it is promoted to the physical model. The physical model is the closest representation to the actual database, and that physical model can generate code to create the physical database on disk. Now in the physical model, entities are called tables, and attributes are called columns. So to recap, we model data to organize and define our data, have common definitions and standards, document business processes, and to generate code to forward engineer a database. This, by the way, is an example of model-driven development. Speaking of forward engineering, the code generated is a type of SQL, standard query language, and is called DDL, which stands for data definition language. This code creates the hotel table, but this part is only a portion of the script. The whole script builds the entire database and all the tables within it. This DDL script is passed on to database administration to deploy and create a physical database that we can actually store data in, which takes us to our next module, Managing and Working with Data, which is all about administration and engineering. I'll see you there.

Module Summary
Okay, here we are. In this module, you learned about data governance and why it's important. You learned about data architecture and its relationship to enterprise architecture, the data model and a brief intro to the relational model and normalization. We also learned that in our physical model, we can generate SQL code and take that script and give it to our DBA to deploy to create an actual database that we store actual data in. Now it's time to get into Managing and Working with Data. I'll see you there.

Managing and Working with Data
Module Introduction
Hello, and welcome back to Big Picture: Enterprise Data Management. I'm Joe Cline. In this module, Managing and Working with Data, we'll get into the administration and engineering aspects of data management. By the end of this module, you'll have learned what database administration is, the DBA and what they do, enforcing data quality through data governance policies, compliance auditing, database development and developing DAOs for the application, and data integration and the development of ETL.

Database Administration and the DBA
At the end of the last module, we talked about promoting the logical model to the physical model and generating data definition language, or DDL, saved as a script to build the actual database. But what do we do with the script? How does the database actually get created? Enter the DBA, or database administrator. DBA is usually the person who runs scripts to create database objects, but they do so much more than that. Some of the tasks DBAs are responsible for are selecting database servers. And I don't just mean the RDBMS; I'm talking about the hardware too. Often, they're engaged in connection to the network, storage, and application access to the database. They run and maintain backups of the database, run the occasional restore test from the backup, automate maintenance tasks and other scheduled jobs, manage database users and group accounts, they secure the data, monitors for performance and troubleshoots issues, maintaining table indexing, suggesting changes to the database schema to increase performance like reducing joins by combining some tables, and more. A DBA uses a SQL editor to interact with the database. It would be the tool that we would use to run our DDL script. Now that the database is created, let's talk database development and the DAO.

Database Development and the DAO
Now, database development can be a dedicated position as part of your application development team, or it could just be an aspect of an application developer's or DBA's job. Either way, somebody needs to write the code that access the database. You can write the code in several ways, but is one better than the other? On one side, you have embedded SQL. On the other, you have a stored procedure. With embedded SQL, you write a class as a data access object, or DAO. On the other side you write a script written in SQL that you run against the database management system that every time you make a call to that stored procedure it runs that logic. So I would consider both of them to be modular and encapsulated. The embedded SQL, however, needs to be recompiled every time there's a change, where the stored procedure doesn't. It's seamless. You can do it hot. With the embedded SQL, could be not as secure. You might be subject to SQL injection attacks. With stored procedures, well, if you write dynamic SQL in a stored procedure, you're defeating the purpose of a stored procedure in the first place. So accessing database objects through stored procedures is usually the preferred method. It also offers a layer of abstraction, so you're accessing a stored procedure and not the underlying tables. Whereas in embedded SQL, unless you're calling a stored procedure, you're writing a query that accesses the tables directly. With embedded SQL, you're doing all your logic in your application. So when you query the database, you have to pull a bigger dataset back to process against, Whereas if you call a stored procedure, all the processing is done on the server side. Then only the data you want gets pulled back to the application, so it reduces network traffic.

Governance - Data Quality and Compliance
When your database is productionalized and data is stored, it becomes subject to data quality and compliance audits. But what does that mean? Now, I mentioned SQL injection attacks in the last clip. One way to avoid that type of thing from happening is adhering to your already established data access coding standards and best practices developed in the data governance process we discussed in the previous section of the course. This is where we want to verify our governance policies are being enforced, so we'll look for things like are data objects meeting our naming conventions? Do columns have the correct data types, and are they consistent throughout the enterprise? Because not all database systems have the same data types. There might be a good reason that the data type is different from one system to the next, but it needs to be documented, and we document that along with our data lineage. All of that is about maintaining quality through adherence to governance policies, but there's more to data quality than that. It's also more than checking null values, though sparsity is an important metric to better understanding your data. It's also about gathering statistics on data values. One example where that might be useful is say you have a column that is a data type that is a date-time type that is granular to the millisecond. By profiling that table, you discover that 88.9 % of the values that are stored in that particular column have the time portion set at midnight, or zeros. It could turn out that the time portion of the date-time column isn't needed, and the data type could be changed to a date data type, not only saving storage, but CPU processing. Now, that column is a candidate for re-evaluation of its data type, through the governance process, of course. A better example is maybe a date was stored as a string that is parsed regularly for date parts or stored in different date formats throughout the enterprise. That definitely needs to be re-evaluated. Okay, now you are getting a better understanding of what it means to know your data and how you can adhere to your company's data governance policies. But are your data governance policies in compliance with not just federal law like HIPAA and Sarbanes-Oxley, but industry rules? The Payment Card Industry Security Council is made up of companies like Visa, Mastercard, JP Morgan Chase, to name a few. These companies came together to establish rules for those businesses who accept credit cards as payment for goods and services. It's a privilege, not a right. These rules are known as PCI for Payment Card Industry or PCI DSS for Data Security Standards. And if you violate them, your company could lose the privilege of being able to accept credit cards. And in this day and age, that could be disastrous, not just for your company's reputation, but it might put the company out of business altogether. Now, compliance auditing is usually done by your company's internal InfoSec or cybersecurity department, or they might bring in an auditing firm to do the job as a self-check. So let's take a quick look at these laws where you might be responsible for the data they are meant to regulate. Let me tell you a little story. This is Bob. This is Bob's store. Bob's store accepts credit cards. Bob's customers are happy. Bob is happy because his customers are happy. Bob doesn't have a policy for his customers' PII and credit card data. Bob gets hacked, and now his customers' data is on the dark web. Bob's customers are not happy and sue for damages. Bob loses his merchant account, goes out of business, and files for bankruptcy. Bob is not happy. Don't be Bob. Some other considerations you should have to prepare for a compliance audit is keeping track of who has database access by logging and saving login attempts, especially failed login attempts, and being able to prove you employ the principle of least privilege, which is to say you only give as much access to the data as people need to do their jobs and nothing more. And how would you prove something like that? Right again, data governance, specifically the change management policy of your data governance program. With change management, you should be tracking every request and fulfillment to the database environment. Basically, the auditor just wants to know who has access and how much privilege that person has and whether or not they need it. And you can simply point the auditor to the change request tickets to compare it with what's actually implemented in the database system. Besides access, they want to know the name of the tables and the columns of the data subjected to the compliance laws. And since we model our data, we can easily show them in the data model and prove it in the database with a simple query and an ER diagram generated from the database system. You'll also need to show a written policy on data retention, which can be a simple blanket policy of you delete everything that's older than six months or a year or a policy specific to each type of data that is subjected to compliance. Just be prepared to back up your rationale for the policy with defined and documented business rules. Again, since we model our data, we can do that too. Data quality and compliance auditing are disciplines in their own right, and establishing a process for them that is built into your data and software development lifecycles can be tricky. So where do we perform quality checks and audits anyway? Well, I could be flip and just say everywhere, but it's probably not ideal to have these assessments at every stop in the flow of the lifecycle. So let's just say data quality and audits should happen at points in the flow where the data is transformed or being used for reporting. What do I mean by transformed? Well, I'm glad you asked because data transformation is one of the topics we'll discuss in the next clip, Data Integration and ETL. I'll see you there.

Data Integration Development and the ETL
Now we have a database in production with data from our imaginary application. Okay, so now what? Are we just going to leave it there and not do anything with it? Of course not, but it wouldn't be wise to perform analysis on data in an OLTP environment, right? You should be nodding your head right now. Let's assume we have already developed a data warehouse going through the same process we did to create our OLTP application database, and it's just sitting there empty. We should be moving that data to the data warehouse. And when I say data warehouse, it could just as easily be a Hadoop data lake on Parquet files in an AWS environment. So how do we get data from the one data store to another? ETL is the answer. ETL stands for extract, transform, and load. Sometimes it's ELT, where we load the data into the target data store and then perform some transformation on it. But usually, we do it on the fly. There are several ways to perform ETL. Some of the more popular off-the-shelf solutions are Informatica; SyncSort's DMX and DMX-h, the h is a newer version that stands for Hadoop; Microsoft's SSIS, that is SQL Server Integration Services, which comes free with the Standard, Business Intelligence, and Enterprise editions of SQL Server; and we have IBM's DataStage. I've used and like both SyncSort's DMX-h, DMX-hs, and Microsoft's SSIS, though I'm sure the others work equally as well. I'm not recommending one over the other. That's just something you'll have to evaluate for yourself. There are open-source solutions too like Talend, Pentaho, Jasper, and the Apache Camel project just to name a few. You can also develop your own home-baked solution with Java or Python or some other language that you normally code in. If you get into data science, you'll likely have to learn to create your own anyway. I personally use Python to build data pipelines for analytics and predictive modeling, but I'll get into that later. When it comes to moving large amounts of data from an OLTP database into a data warehouse or into Hadoop, I'm going to use an ETL solution. Now keep in mind what I'm going to describe next is a condensed in a nutshell type example of how we might create an ETL package. These programs can be fairly intuitive, but they can also get really complicated if you need to do some serious data munging. But since we were good little boys and girls and did our due diligence with data governance, architecture, and data modeling, it'll be a lot easier than if we had not done those things. Okay, once you have your ETL tool or custom ETL program, you'll connect to the source data tables usually through an ODBC or JDBC driver or a REST API if it's a service, and you'll pull a result data set back to your local machine, or if you're using an enterprise solution, a dedicated ETL server. While the data is being piped over the wire, you then have code to perform some transformation as it arrives in memory. It can be as simple as copy or as complex as the business requires. But common transformations are unions, joins, casting different data types, substring parsing with built-in string functions, or custom regex code. Regex stands for regular expressions, and it's a combination of characters that you put together to define a generic pattern which is used by a search algorithm to find or exclude a string from your document or text field. Using regex can be a little intimidating at first, but you'll get used to it the more you use it. Okay, so we connected to our data source, and while it is sending data our ETL package performs some type of transformation on it. Next, we send our transformed records to a target data store, probably again through ODBC or a JDBC driver connection or a REST API. Once the package is finished, it has moved and reshaped the data into a form that we can use for analysis by inserting it into an OLAP cube, a data warehouse, or a data lake. And that's that. We now have data in a place and shape where we can perform business intelligence which just scratches the surface because most businesses are just now realizing there's gold in them thar hills of data. We'll have to do a bit more data wrangling to prepare it for data mining through statistical analysis and predictive modeling, which we'll get into in an upcoming section of the course.

Module Summary
In this module, you learned what database administration is and what a DBA does, where we can enforce data governance policies, perform data quality checks, and compliance auditing. You then learned a little bit about database development and the data access object. And finally, you learned about the data integration developer and the ETL programs that they create to move large amounts of data and transform it on the fly from one place to another. Now join me in the next section where we will work on getting value from data with business intelligence, exploratory and statistical analytics, predictive modeling, and data visualization. I'll see you there.

Getting Value from Data
Module Introduction
Welcome back to Big Picture: Enterprise Data Management. I'm Joe Cline. In this module, we'll explore the data management disciplines for getting value from your data. We'll start with data engineering, sometimes called big data engineering, and discuss how it's more than ETL development and why what data engineers do is sometimes referred to as data munging or data wrangling. Next, we'll look at business intelligence, or simply BI, what the BI developer's responsibilities are, and some of the tools they use. We'll then get into the disciplines that have come to be associated with the term data science, EDA, or exploratory data analytics; statistical data analytics, the core of data science; predictive modeling with machine learning algorithms; and data visualization. But before we get into data engineering, let's talk a little bit about big data and what that means. I'll see you in the next clip.

Big Data
Okay, so for this being a course on enterprise data management, you might find it kind of strange that we have yet to talk about big data. You probably have heard of big data. Maybe you don't understand what that means or what the difference is between big data and small data or medium data, so I'll just run down a slide real quick here and give you kind of a consensus of the definition of what big data is. And that consensus has come to what's known as the five V's of big data, and it's these V's that describe what big data is. So let's start off with the first one is volume. And what that means is that there's a lot of data, meaning greater than a terabyte. And if you think about it, it makes sense because usually big data technology does good with data that's less than a terabyte. The next V is variety, and what that means is we're getting structured data and unstructured data. We're getting schema data, we're getting schema list data, and we have to figure out how we're going to reshape that so we can work with it. Then there's velocity. That means more data is coming at you all the time, and you've got to be able to handle that. Variability means you're getting data from all kinds of different sources, and your job as a data engineer is to find the data that complements your other data. So, for example, if I have travel data, well, I might be interested in weather data and combining that. So that's what variability means. You need to have different kinds of data, not necessarily data that you generated. Then there's veracity, meaning that there's potential data issues. And if you're mixing data from within your company with outside sources, maybe it's other companies, and so now you have data governance standards, you have naming standards, and you have everything defined, but that doesn't mean every company has done that. And so you're going to have to deal with that. You're going to have to figure out what their data means sometimes and see how it matches to yours. But the good news is you can go through that data governance exercise with that outside data and reshape it to your needs. Okay, so that's the definition of big data. And so now that we have a better understanding of what big data is, let's continue on with the rest of data engineering.

Data Engineering
So you're probably wondering, hey, Joe, what is data engineering? Isn't that the same as data integration? Can I just use that ETL tool you were talking about earlier? Well, yeah, sure. You can use an ETL tool, but it really depends on a few factors like how much data you're moving, what you're trying to do with the data, how disparate are the sources, can your ETL tool access web services, scrape websites, connect to the cloud, can it access Hadoop, and how permanent will it be? Why? Because data engineering is about building data pipelines quickly as proofs of concept, or POCs, which are almost always temporary. And then the POC is productionalized. Sometimes that happens with a different technology or language. And yes, productionalized is a real word, even if it only exists in the tech world. It just means to take a proof of concept and make it resilient and scalable so it can be put into a production environment. However, before that happens, you'll want to prove something quickly, and maybe it's to answer a question in the data or build an ad hoc model. For example, you might have to prove some logic to combine certain data in Python, and if that's successful, then you might want to productionalize it in an ETL tool or Java. Data integration developers use ETL tools almost exclusively while data engineers use a number of tools like R, Python, SAS, Java, definitely SQL, Scala is great for Hadoop environment, Go has become pretty popular, and Julia, a functional programming language. Where ETL tools are great for moving huge amounts of data, their built-in functionality is pretty basic. Programming languages, on the other hand, are more robust, so you can get complicated business logic written and tested quickly. Other technologies a data engineer might use, along with the languages I mentioned earlier, are the columnar database. Maybe your company uses a SAS server environment and their client tools like Enterprise Guide or Enterprise Data Miner; RStudio, which is an IDE, or integrated development environment, for the R language; Anaconda Enterprise server, which began as a Python thing, but they've started integrating other languages like R and Go; the Hadoop ecosystem and all it entails. So where does all this fit into our data lifecycle? Let's take a look at our generic architecture again. Now I grayed out the part we've already gone over, and as you can see from our pipeline, we can take input data from anywhere. Of course, there are the usual sources like OLTP relational databases, data warehouses, data lakes, OLAP cubes, but we can also get data from log files, NoSQL databases, REST APIs, and in the cloud where a lot of companies build their Hadoop solution in like say Amazon Web Services. I don't have it in this example, but we can also collect data from things as in the Internet of Things like sensors, like from your television or thermostat or from your refrigerator or other smart devices. You can even get data from Alexa. The pipeline continues with some kind of munging and outputs the resulting dataset into an analytics database, like I said, Vertica or SQL Server, or an analytics environment like SAS. From there, you would use a language like Python or R or a spreadsheet program like Excel to pull data into memory and clean it up to create tidy data. Tidy data is a result of the final cleaning process before you perform analysis. So you can make sure you don't have duplicates from erroneous joins or filling in nulls or empty string values by imputing existing data like using zeros or taking an average of all values for numeric fields or entering 01-01-1970 for missing dates, for example. The idea is to clean the data so you can perform statistical analysis without missing values, screwing up the results. This is another good place to implement data quality checks before reporting your final analysis. All of what I just described is primarily for EDA, statistical analytics, or predictive modeling. Business intelligence developers would typically pull data from a data warehouse, an OLAP cube for the backup stream. So let's take a look in our next clip, Business Intelligence Reporting. I'll see you there.

Business Intelligence
The BI developer isn't your typical programmer. In fact, some may have never written a line of code, though many of the most popular BI software packages, such as Microsoft's SQL Server Reporting Services, or SSRS, BusinessObjects Crystal Reports, or Tableau have some capacity for developing custom reports that might be a little more complex than what you can do with their built-in drag and drop tools. The BI system you are working with will likely determine which programming language you'll use to write custom code for complex reports. For example, for SSRS, you'll probably use C#. Crystal Reports uses XML configuration files. And though Tableau may not allow you to write code directly for the reporter dashboard, they do provide an SDK, or software development kit, for several programming languages so you can engineer and munge data to create a dataset that then can be published to the Tableau Server. For the most part though, you can create very nice reports with their drag and drop features and their provided database connections. With that, the BI developer, in conjunction with the business stakeholder, will be responsible for designing the report layout and making connections to data sources, possibly writing custom SQL to develop the dataset for the report. If you're connecting to a data warehouse or OLAP cubes, which might be what your data warehouse looks like, just a bunch of OLAP cubes, you can also create interactive reports that allow you to drill in to a finer set of data for a particular metric. You can even slice by a certain attribute, like grouping the data by city, state, or year. Here we have a typical BI data flow. Obviously, it's a highly simplified diagram and more conceptual than a physical implementation would be. Now look to the middle right. We have our BI tool where we design the report and create our database connection. This group that represents code could be custom or just the automatically generated code from the BI tool. Either way, we connect to the data source with some kind of query and we get back the result set, which could be further refined before displaying the report in the dashboard. We then publish our report or dashboard to a content management server, which could be like SSRS, BO, or Tableau Server, or another enterprise content management server like Joombla, Plone, or Magenta, or one of the many wiki solutions out there. Let's take a step further and look at data mining our mountain of data. And in our next clip, Exploratory and Statistical Analytics, we'll see what it means to do just that.

Exploratory and Statistical Data Analytics
So who does exploratory data analytics? Well, it's anyone who wants to understand their part of the business better. Usually though it's your company's subject matter experts, or SMEs. They can be from any department that collects internal data. E-commerce, who wants to analyze click-through rates or patterns. And there's sales, who wants to examine previous sales data to look for patterns to increase their numbers. And call centers want to understand how they can improve their call wait times or reduce their abandonment rate. And then there's marketing, who wants to compare cohorts in demographic data, analyze A/B testing result, or perform sentiment analysis. But what do they do to perform this data analysis? Well, sometimes it starts just by poking around the data, methodically, of course. That exploration could expose a pattern which might drive the SME to perform a what if analysis. It's at this point where data mining really starts. So here's another story, but this time we have Jamal who is our SME in the marketing department. Jamal performs exploratory data analysis and finds a curious pattern in his customer relationship management data. Jamal tries a what if with a few values that further his curiosity. Jamal wants to use data science techniques to create an automated customer profiler tool, but Jamal doesn't have the skill set to build it himself. Jamal goes to Maria, a data scientist at his company, for help. Maria needs to incorporate data from other datasets, but doesn't know how to get all the various internal and external data formats together. Maria asks Lakshmi, a data engineer, to help her. Lakshmi develops some API clients and web scraper scripts to wrangle external data and munge them together with internal CRM data to create a pipeline to ingest into the data lake. Maria uses the newly combined data to train, test, and build a predictive algorithm, which, with Lakshmi's help, is then productionalized to run as a scheduled job once a week. Jamal, Maria, and Lakshmi together create a data visualization graphic which is consumed by the company's internal marketing dashboard. The project was so successful that Jamal got a promotion, and Maria and Lakshmi, they got company bonus awards for their part in the project. Jamal is happy. Maria is happy. Lakshmi is happy. Statistical data analysis. This is the part I consider to be the core of data science because you use the scientific method, you use sampling techniques to mitigate bias, you test hypotheses looking for any significant response from those tests. So, who qualifies as a data scientist? Well, so far, companies had been hard pressed to find the definitive data scientist. They are so rare in fact, they are referred to as unicorns. Why are they so rare? Well, because it's a mix of skills that's hard to find in any one person. The definitive data scientist has to be a statistician or other type of formally trained scientist, plus a data engineer, plus a business analyst, plus a graphic designer to create awesome infographics, plus they have to be a people person, like in sales or marketing, because they have to be able to present their findings to people who may not understand the technical stuff behind it. They are usually required to have a master's or a PhD in a quantitative field. So if data scientists are so rare, what are companies to do? Well, the first thing to do is lower expectations. Not every job candidate needs to have a PhD, nor do they necessarily have to have all of the qualities I just mentioned. More and more companies are building data science teams to compensate where the team members' combined skill sets meet that of the data scientist definition. We now know what exploratory data analysis and statistical analysis is and who does it, but you might be thinking, what about all this machine language stuff I've been hearing about? Isn't that data science too? Well, let's talk about it in the next video, predictive modeling. I'll see you there.

Predictive Analytics
So what is this predictive modeling? Well, you can think of it as estimating an outcome by looking at patterns in your existing data, and you can do that by choosing certain variables from a dataset to be your predictors. That's the modeling part. The data for those predictors is run through a statistical algorithm that will assess, rate, and rank those variables in determining the accuracy of the output. Now you might have to modify your model a few times and try a few different strategies to increase the accuracy, but fortunately there are many ways to do that. The algorithms I'm referring to are machine learning algorithms, and there are a number of them that do different things depending on the question you're trying to answer and the data that you're working with. For example, there is supervised learning where you split your sample data into a training dataset and a test dataset. You use the training dataset while developing your model and the test dataset to validate your model. Here are a few common supervised machine learning algorithms: decision trees, Naive Bayes classification, linear or logistic regression, support vector machines, or SVM, and this is just to name a few. There's a bunch more. Well, if you have supervised learning, then you must have unsupervised learning where you don't use a training or test dataset. You'll want to use one of these if you're trying to do some anomaly detection or some kind of clustering. A few common clustering algorithms are k-means and k-NN, also known as nearest neighbor. So there's supervised and there's unsupervised, but there's also semi-supervised, reinforced and deep learning. Now you can do predictive learning in paid solutions like SAS or IBM's SPSS or learn some free open-source languages like Python and R and use some of their packages. Some of the common packages for those languages are TensorFlow, scikit-learn, and caret to name a few. Well, now you have your results from your exploratory data analysis to discover value, statistical data analysis to understand and show evidence of the value, and now you have predictive modeling to see how you can use that value to make educated business decisions. But how do you show a board of directors or a group of investors what all this means? With data visualization, of course, and that's our next clip. I'll see you there.

Data Visualization
I hear you asking, did someone just come up with the term data visualization to make slide decks sound more interesting to watch? Well, it's true. We've all seen our share of slide deck presentations that can be a little boring. In fact, you're probably looking at one right now, though I hope it's been way more entertaining and educational than the ones you've seen before. But to answer the question, no. Today, you have to wow them visually and give them an exciting way to consume the information you're trying to present to them. Basically, you want to tell a story, and one very popular way of doing this is with infographics, which is a picture that makes your eyes follow a path around a page to see how data compares and contrasts with other data. There are some neat tools out there to help you make infographics, and I listed some of them here, like Canva and Venngage, and then there's Pikochart. Another exciting way to tell your data story is with 3D, virtual or augmented reality and, of course, animation. If you've never seen a data story being told with data visualization techniques, check out Hans Rosling's 200 Countries, 200 Years, 4 Minutes. It's a short, approximately 4-minute video, I'm sure you'll find inspiring. Some common programming language libraries for visualization are Matplotlib, Bokeh, Plotly, and ggplot2. And you can even get creative if you're good with JavaScript in D3.js. Well, that's about it for this module. In the next clip, I'll sum it all up for you.

Module Summary
In this module, you were introduced to the topics of data engineering and big data, business intelligence, exploratory data analytics, statistical data analytics, predictive modeling, and data visualization. Next, in the final module, we'll sum it all up and talk about where to go from here. I'll see you in the next clip.

Course Summary
Thank You
Well this is it, the end of the course. Now if you made it this far, you should congratulate yourself as we've covered a lot of information within the data management space in a very short period of time. Let's take a look at how far we've come. Even though we've covered a lot, we've done it at a very high level, but we talked about the data lifecycle and DAMA, big data and Hadoop, blockchain technology and how that might fit into the greater enterprise architecture, the various roles throughout the data management space, some methods, as well as the many technologies used by data management professionals, and the various disciplines they specialize in, like data governance, data architecture, data modeling, database and data warehouse administration, data development, data integration, data engineering, business intelligence, exploratory data analysis, statistical data analysis, predictive modeling and machine languages, and finally data visualization. I want to say thank you for taking Big Picture: Enterprise Data Management. I hope you found it as fun as I did putting it together. But more importantly, I hope you found some value in it. So where do we go from here? Well, if you found any of the particular disciplines or topics discussed in this course interesting and want to learn more about them, stay tuned for more courses here on Pluralsight where I plan to get into a deep dive for each one. And if you have any suggestions on a course topic or have a question or comment about a course, shoot be a tweet @mrjoedata. I'd love to hear from you. You can also see what I'm up to on my blog, datanomicon.blog. That's .b -l-o-g. I'm Joe Cline, and until next time, go model something awesome.
